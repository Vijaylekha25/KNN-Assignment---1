{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23fb51f-fec5-4d9b-ad6f-0ef924ffce57",
   "metadata": {},
   "source": [
    "# What is the KNN algorithm?\n",
    "K-nearest neighbors (KNN) is a simple supervised machine learning algorithm used for classification and regression tasks. It classifies a new data point based on the majority class of its K nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417e94d-81c8-4ff6-9a8e-f90d31b34357",
   "metadata": {},
   "source": [
    "# How do you choose the value of K in KNN?\n",
    "The value of K in KNN is typically chosen through cross-validation techniques such as grid search or by using domain knowledge. A smaller K value leads to more flexible decision boundaries but increases the noise in the classification, while a larger K value leads to smoother decision boundaries but may overlook local patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffe166-e840-4d9f-a1e8-c13a861a1036",
   "metadata": {},
   "source": [
    "# What is the difference between KNN classifier and KNN regressor?\n",
    "KNN classifier predicts the class of a data point by a majority vote of its neighbors, while KNN regressor predicts the output value for a data point as the average of the values of its nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024dc779-8375-4f9c-9a68-9a3949f428b7",
   "metadata": {},
   "source": [
    "# How do you measure the performance of KNN?\n",
    "The performance of KNN can be measured using metrics such as accuracy, precision, recall, F1-score for classification tasks, and mean squared error (MSE), R-squared for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed414a-f373-4d16-b1c6-6a056014f7a1",
   "metadata": {},
   "source": [
    "# What is the curse of dimensionality in KNN?\n",
    "The curse of dimensionality refers to the phenomenon where the performance of KNN deteriorates as the number of dimensions (features) increases. This is because in high-dimensional spaces, the concept of 'nearest neighbors' becomes less meaningful, as the distance between points becomes more uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4393c0c7-816b-40af-918c-840a5fdc4f71",
   "metadata": {},
   "source": [
    "# How do you handle missing values in KNN?\n",
    "Missing values can be handled in KNN by either imputing them with the mean, median, or mode of the feature or using algorithms specifically designed for imputing missing values such as k-nearest neighbors imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8467cdc9-6bb1-4d1d-a23e-ca2d61015fb2",
   "metadata": {},
   "source": [
    "# Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "KNN classifier is better suited for classification problems where the output is categorical, while KNN regressor is better suited for regression problems where the output is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b49152-1ae6-43dd-b9da-a4ba225bd0f5",
   "metadata": {},
   "source": [
    "# What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "Strengths: Simple to implement, doesn't require training time, works well with small datasets.\n",
    "Weaknesses: Computationally expensive during testing, sensitive to irrelevant features, requires proper scaling of features.\n",
    "These weaknesses can be addressed by using dimensionality reduction techniques, feature selection, and feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb17846-bcdf-4780-bb5c-41e7ed266dde",
   "metadata": {},
   "source": [
    "# What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Euclidean distance is the straight-line distance between two points in the feature space, while Manhattan distance is the sum of the absolute differences between the coordinates of the two points. Euclidean distance is sensitive to the magnitude of differences, while Manhattan distance is sensitive to the direction of differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e83e6-1f00-4b0f-b5be-5d2e834fddd1",
   "metadata": {},
   "source": [
    "# What is the role of feature scaling in KNN?\n",
    "Feature scaling is important in KNN because it ensures that all features contribute equally to the distance calculations. Without scaling, features with larger magnitudes may dominate the distance calculations, leading to biased results. Common techniques for feature scaling include standardization (subtracting mean and dividing by standard deviation) and normalization (scaling features to a range, typically [0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab612c-2893-47fc-b978-90d2843ee955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7020dc-6b08-4eff-8b0f-fe1179e186bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
